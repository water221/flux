# å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼šå®ç°ä¸‰ä¸ªåˆ›æ–°ç‚¹

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ é€æ­¥å®ç°ä¸‰ä¸ªåˆ›æ–°ç‚¹ï¼Œæå‡ Diff2Flow åœ¨ img2depth ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

---

## ğŸ“‹ å‰ç½®è¦æ±‚

1. **ç¯å¢ƒè®¾ç½®**
   ```bash
   conda env create -f environment.yml
   conda activate flux
   ```

2. **å®‰è£…é¢å¤–ä¾èµ–**
   ```bash
   pip install transformers timm  # ç”¨äº DINOv2
   ```

3. **ä¸‹è½½æ¨¡å‹æƒé‡**
   - DINOv2 ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½
   - ç¡®ä¿ Stable Diffusion æƒé‡å·²ä¸‹è½½

---

## ğŸš€ å®æ–½æ­¥éª¤

### Step 1: å®ç°å‡ ä½•ä¸€è‡´æ€§çº¦æŸï¼ˆæ¨èå…ˆå®ç°ï¼‰

#### 1.1 åˆ›å»ºå‡ ä½•æŸå¤±æ¨¡å—

åˆ›å»ºæ–‡ä»¶ `diff2flow/losses/__init__.py`:
```python
from .geometry_loss import GeometryConsistencyLoss
```

åˆ›å»ºæ–‡ä»¶ `diff2flow/losses/geometry_loss.py`:
ï¼ˆå¤åˆ¶åˆ›æ–°ç‚¹åˆ†æ.md ä¸­çš„å®Œæ•´ä»£ç ï¼‰

#### 1.2 ä¿®æ”¹è®­ç»ƒæ¨¡å—

åœ¨ `diff2flow/trainer_module.py` ä¸­ï¼š

1. å¯¼å…¥å‡ ä½•æŸå¤±ï¼š
```python
from diff2flow.losses.geometry_loss import GeometryConsistencyLoss
```

2. åœ¨ `__init__` æ–¹æ³•ä¸­æ·»åŠ ï¼š
```python
self.use_geometry_loss = cfg.get('use_geometry_loss', False)
self.geometry_loss_weight = cfg.get('geometry_loss_weight', 0.1)
if self.use_geometry_loss:
    self.geometry_loss = GeometryConsistencyLoss()
```

3. åœ¨ `training_step` æ–¹æ³•ä¸­æ·»åŠ å‡ ä½•æŸå¤±è®¡ç®—ï¼š
```python
# åœ¨è®¡ç®—ä¸»æŸå¤±å
if self.use_geometry_loss and self.current_epoch > 0:
    # è§£ç  latent åˆ°æ·±åº¦ç©ºé—´
    with torch.no_grad():
        depth_pred_latent = self.model.compute_xt(x0=torch.randn_like(z), x1=z, t=torch.ones(z.shape[0], device=z.device))
        depth_gt_latent = z
    
    # è§£ç åˆ°åƒç´ ç©ºé—´ï¼ˆéœ€è¦ first_stageï¼‰
    depth_pred = self.first_stage.decode(depth_pred_latent)
    depth_gt = self.first_stage.decode(depth_gt_latent)
    
    # è®¡ç®—å‡ ä½•æŸå¤±
    image = batch.get('x0', None)
    valid_mask = batch.get('valid_mask', None)
    geom_loss, geom_dict = self.geometry_loss(
        depth_pred, depth_gt, valid_mask, image
    )
    
    loss = loss + self.geometry_loss_weight * geom_loss
    
    # è®°å½•
    for k, v in geom_dict.items():
        self.log(f'train/{k}', v, on_step=True, on_epoch=True)
```

#### 1.3 åˆ›å»ºé…ç½®æ–‡ä»¶

åˆ›å»º `configs/experiment/img2depth/obj_base_geometry.yaml`:
```yaml
# @package _global_
defaults:
  - override /model: obj_ch8
  - override /data: dummy_depth
  - override /lora: null
  - override /task: img2depth

name: img2depth/obj/base/geometry

use_geometry_loss: True
geometry_loss_weight: 0.1
```

#### 1.4 è¿è¡Œè®­ç»ƒ

```bash
python train.py experiment=img2depth/obj_base_geometry data=hypersim
```

---

### Step 2: å®ç°å¤šå°ºåº¦ç‰¹å¾èåˆ

#### 2.1 åˆ›å»ºå¤šå°ºåº¦ç¼–ç å™¨

åˆ›å»ºæ–‡ä»¶ `diff2flow/conditioning/multiscale_encoder.py`:
ï¼ˆå¤åˆ¶åˆ›æ–°ç‚¹åˆ†æ.md ä¸­çš„å®Œæ•´ä»£ç ï¼‰

#### 2.2 ä¿®æ”¹ä»»åŠ¡é…ç½®

åˆ›å»º `configs/task/img2depth_multiscale.yaml`:
```yaml
name: img2depth_multiscale
context_key: x0_latent
conditioning_key: x0
cond_stage_cfg:
  target: diff2flow.conditioning.multiscale_encoder.MultiScaleImageEncoder
  params:
    model_name: dinov2_vitb14
    feature_layers: [-4, -3, -2, -1]
    output_dim: 768
    freeze_backbone: True
cond_dropout: 0.0
metric_tracker_cfg:
  target: diff2flow.metrics.DepthMetricTracker
visualizer:
  target: diff2flow.visualizer.ImageDepthVisualizer
```

#### 2.3 ä¿®æ”¹è®­ç»ƒæ¨¡å—ä»¥æ”¯æŒå¤šå°ºåº¦ç‰¹å¾

åœ¨ `diff2flow/trainer_module.py` çš„ `forward` æ–¹æ³•ä¸­ï¼š
```python
def forward(self, x, t, **kwargs):
    # å¦‚æœæ¡ä»¶ç¼–ç å™¨æ˜¯å¤šå°ºåº¦ç¼–ç å™¨
    if hasattr(self.cond_stage_model, 'forward') and 'x0' in kwargs:
        images = kwargs['x0']
        global_feat, multi_scale_feat = self.cond_stage_model(images)
        kwargs['multiscale_features'] = multi_scale_feat
        kwargs['global_feature'] = global_feat
    
    return self.model(x, t, **kwargs)
```

#### 2.4 è¿è¡Œè®­ç»ƒ

```bash
python train.py experiment=img2depth/obj_base task=img2depth_multiscale data=hypersim
```

---

### Step 3: å®ç°ä¸ç¡®å®šæ€§æ„ŸçŸ¥

#### 3.1 ä¿®æ”¹ UNet æ¨¡å‹

åœ¨ `diff2flow/models/unet/openaimodel.py` ä¸­ï¼š

1. åˆ›å»º `UncertaintyAwareUNetModel` ç±»ï¼ˆç»§æ‰¿ `UNetModel`ï¼‰
2. æ·»åŠ ä¸ç¡®å®šæ€§é¢„æµ‹å¤´
3. ä¿®æ”¹ `forward` æ–¹æ³•è¿”å›ä¸ç¡®å®šæ€§

ï¼ˆå‚è€ƒåˆ›æ–°ç‚¹åˆ†æ.md ä¸­çš„å®Œæ•´ä»£ç ï¼‰

#### 3.2 ä¿®æ”¹ FlowModelObj

åœ¨ `diff2flow/flow_obj.py` ä¸­ï¼š

1. åˆ›å»º `UncertaintyAwareFlowModelObj` ç±»
2. å®ç° `sample_vt_with_uncertainty` æ–¹æ³•
3. ä¿®æ”¹ `training_losses` ä½¿ç”¨å¼‚æ–¹å·®æŸå¤±

ï¼ˆå‚è€ƒåˆ›æ–°ç‚¹åˆ†æ.md ä¸­çš„å®Œæ•´ä»£ç ï¼‰

#### 3.3 åˆ›å»ºé…ç½®æ–‡ä»¶

åˆ›å»º `configs/model/obj_ch8_uncertainty.yaml`:
```yaml
start_from_noise: True
noising_step: -1
fm_cfg:
  target: diff2flow.flow_obj.UncertaintyAwareFlowModelObj
  params:
    sigma_min: 0.0
    schedule: linear
    uncertainty_weight: 1.0
    min_uncertainty: 1e-3
    net_cfg:
      target: diff2flow.models.unet.openaimodel.UncertaintyAwareUNetModel
      params:
        in_channels: 8
        predict_uncertainty: True
        load_from_ckpt: checkpoints/v2-1_768-ema-pruned.ckpt
```

#### 3.4 è¿è¡Œè®­ç»ƒ

```bash
python train.py experiment=img2depth/obj_base model=obj_ch8_uncertainty data=hypersim
```

---

## ğŸ¯ å®Œæ•´é›†æˆï¼ˆæ‰€æœ‰åˆ›æ–°ç‚¹ï¼‰

### åˆ›å»ºå®Œæ•´é…ç½®æ–‡ä»¶

åˆ›å»º `configs/experiment/img2depth/obj_enhanced.yaml`:
```yaml
# @package _global_
defaults:
  - override /model: obj_ch8_uncertainty
  - override /data: dummy_depth
  - override /lora: null
  - override /task: img2depth_multiscale

name: img2depth/obj/enhanced

# å‡ ä½•æŸå¤±
use_geometry_loss: True
geometry_loss_weight: 0.1

# ä¸ç¡®å®šæ€§æƒé‡
uncertainty_weight: 1.0
```

### è¿è¡Œå®Œæ•´è®­ç»ƒ

```bash
python train.py experiment=img2depth/obj_enhanced data=hypersim
```

---

## ğŸ“Š éªŒè¯å®ç°

### æ£€æŸ¥ç‚¹

1. **å‡ ä½•æŸå¤±**
   ```python
   # åœ¨è®­ç»ƒæ—¥å¿—ä¸­åº”è¯¥çœ‹åˆ°
   train/geometry_normal
   train/geometry_gradient
   train/geometry_total
   ```

2. **å¤šå°ºåº¦ç‰¹å¾**
   ```python
   # æ£€æŸ¥ DINOv2 æ˜¯å¦åŠ è½½
   # æ£€æŸ¥ç‰¹å¾ç»´åº¦æ˜¯å¦æ­£ç¡®
   ```

3. **ä¸ç¡®å®šæ€§**
   ```python
   # æ£€æŸ¥ä¸ç¡®å®šæ€§è¾“å‡ºå½¢çŠ¶
   # æ£€æŸ¥ä¸ç¡®å®šæ€§å€¼æ˜¯å¦åˆç†ï¼ˆ> 0ï¼‰
   ```

### å¯è§†åŒ–

ä¿®æ”¹ `diff2flow/visualizer.py` æ·»åŠ ä¸ç¡®å®šæ€§å¯è§†åŒ–ï¼š

```python
def visualize_uncertainty(self, uncertainty, save_path):
    """å¯è§†åŒ–ä¸ç¡®å®šæ€§å›¾"""
    import matplotlib.pyplot as plt
    
    uncertainty_np = uncertainty.detach().cpu().numpy()
    plt.figure(figsize=(10, 5))
    plt.imshow(uncertainty_np[0, 0], cmap='hot')
    plt.colorbar()
    plt.title('Uncertainty Map')
    plt.savefig(save_path)
    plt.close()
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: DINOv2 ä¸‹è½½å¤±è´¥
**A**: æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼š
```python
from transformers import AutoModel
model = AutoModel.from_pretrained("facebook/dinov2_vitb14", cache_dir="./models")
```

### Q2: å†…å­˜ä¸è¶³
**A**: 
- å‡å°‘ batch size
- å†»ç»“ DINOv2 backbone
- ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

### Q3: å‡ ä½•æŸå¤±ä¸º NaN
**A**:
- æ£€æŸ¥æ·±åº¦å€¼èŒƒå›´
- æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
- é™ä½å‡ ä½•æŸå¤±æƒé‡

### Q4: ä¸ç¡®å®šæ€§è¿‡å¤§
**A**:
- è°ƒæ•´ `min_uncertainty`
- é™ä½ `uncertainty_weight`
- æ£€æŸ¥æŸå¤±å‡½æ•°å®ç°

---

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜å»ºè®®

1. **é€æ­¥å¯ç”¨åˆ›æ–°ç‚¹**
   - å…ˆåªå¯ç”¨å‡ ä½•æŸå¤±ï¼Œè®­ç»ƒç¨³å®šåå†æ·»åŠ å…¶ä»–

2. **è¶…å‚æ•°æœç´¢**
   ```python
   # å‡ ä½•æŸå¤±æƒé‡: [0.05, 0.1, 0.2]
   # ä¸ç¡®å®šæ€§æƒé‡: [0.5, 1.0, 2.0]
   # å­¦ä¹ ç‡: [1e-5, 3e-5, 5e-5]
   ```

3. **å­¦ä¹ ç‡è°ƒåº¦**
   - ä½¿ç”¨ warmup
   - ä½™å¼¦é€€ç«
   - åœ¨éªŒè¯é›†ä¸Šæ—©åœ

4. **æ•°æ®å¢å¼º**
   - éšæœºè£å‰ª
   - é¢œè‰²æŠ–åŠ¨
   - æ·±åº¦å½’ä¸€åŒ–

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. **å®éªŒéªŒè¯**
   - åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•
   - è¿›è¡Œæ¶ˆèå®éªŒ
   - å¯¹æ¯”åŸºçº¿æ–¹æ³•

2. **è®ºæ–‡æ’°å†™**
   - è®°å½•å®éªŒç»“æœ
   - åˆ†æå„ç»„ä»¶è´¡çŒ®
   - å‡†å¤‡å¯è§†åŒ–ææ–™

3. **ä»£ç ä¼˜åŒ–**
   - æé«˜è®¡ç®—æ•ˆç‡
   - æ·»åŠ æ›´å¤šæ³¨é‡Š
   - å®Œå–„æ–‡æ¡£

---

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸš€

